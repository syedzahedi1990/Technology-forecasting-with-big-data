{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1m853dv8jIpeP2m5XJNtm5KG73foXUcWe",
      "authorship_tag": "ABX9TyOZAPdFFaGiO2OYgIKTT+V+"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iMc1v74UbYL"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from __future__ import annotations\n",
        "import datetime as dt\n",
        "import json, math, os, warnings\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import copy\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "import torch\n",
        "print(f\"CUDA available in PyTorch: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is accessible to PyTorch despite NVML warning\")\n",
        "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from scipy.stats import t\n",
        "\n",
        "\n",
        "pm = None; ETSModel = None; Holt = None; SimpleExpSmoothing = None\n",
        "ConvergenceWarning = Warning; ValueWarning = Warning\n",
        "\n",
        "try:\n",
        "    from statsmodels.tsa.exponential_smoothing.ets import ETSModel\n",
        "    from statsmodels.tsa.api import Holt, SimpleExpSmoothing\n",
        "    from statsmodels.tools.sm_exceptions import ConvergenceWarning, ValueWarning\n",
        "except ImportError:\n",
        "    print(\"Warning: statsmodels not fully installed or outdated.\")\n",
        "\n",
        "try:\n",
        "    import pmdarima as pm\n",
        "except (ImportError, ValueError) as e:\n",
        "    print(f\"\\nWarning: Failed to import pmdarima. Auto-ARIMA baseline will be skipped.\")\n",
        "    pm = None\n",
        "\n",
        "mrmr = None\n",
        "try:\n",
        "    import mrmr\n",
        "except ImportError:\n",
        "    print(\"Warning: mrmr-selection not installed. Multivariate mode will fail if used.\")\n",
        "\n",
        "try:\n",
        "    import optuna\n",
        "    from optuna.samplers import TPESampler\n",
        "    from optuna.pruners import SuccessiveHalvingPruner\n",
        "except ImportError:\n",
        "    print(\"Warning: Optuna not installed. The main execution block will be skipped.\")\n",
        "    optuna = None\n",
        "\n",
        "\n",
        "ROOT = Path(\"yourpath\")\n",
        "\n",
        "BASE_OUT = GDRIVE_ROOT / \"36_forecast_outputs_GRU_rec_1stepopt\"\n",
        "BASE_OUT.mkdir(parents=True, exist_ok=True)\n",
        "STORAGE = f\"sqlite:///{BASE_OUT / 'lstm_rec1stepopt_optuna_study.db'}\"\n",
        "MODEL_TYPE = \"GRU_Rec1StepOpt\"\n",
        "\n",
        "\n",
        "FORECAST_HORIZON = 36; SEASONAL_PERIOD = 12\n",
        "\n",
        "\n",
        "OPTUNA_N_TRIALS = 50; MAX_EPOCHS_OPTUNA = 30\n",
        "PATIENCE_ES = 5; N_CV_SPLITS = 5\n",
        "\n",
        "VALIDATION_SIZE_HPO = 36\n",
        "\n",
        "\n",
        "MAX_EPOCHS_FINAL = 150; WEIGHT_DECAY = 1e-4; RANDOM_STATE = 42\n",
        "\n",
        "\n",
        "MC_SAMPLES_FOR_CI = 100; EPSILON = 1e-8\n",
        "\n",
        "\n",
        "if torch: torch.manual_seed(RANDOM_STATE)\n",
        "np.random.seed(RANDOM_STATE)\n",
        "device = torch.device(\"cuda\" if torch and torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "    if torch:\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        if torch.cuda.is_available():\n",
        "            if hasattr(torch.backends.cuda.matmul, 'allow_tf32'):\n",
        "                    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "            if hasattr(torch, 'set_float32_matmul_precision'):\n",
        "                torch.set_float32_matmul_precision(\"medium\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "FUTURE_EXOG_POLICY = \"persist\"\n",
        "\n",
        "\n",
        "if nn:\n",
        "    class GRUForecaster(nn.Module):\n",
        "        \"\"\"\n",
        "        GRU model adapted for 1 step ahead forecasting.\n",
        "        \"\"\"\n",
        "        def __init__(self, input_size: int, hidden_size: int,\n",
        "                     output_size: int, dropout: float, num_layers: int):\n",
        "            super().__init__()\n",
        "\n",
        "\n",
        "            self.rnn = nn.GRU(\n",
        "                input_size,\n",
        "                hidden_size,\n",
        "                num_layers=num_layers,\n",
        "                batch_first=True,\n",
        "                dropout=dropout if num_layers > 1 else 0.0,\n",
        "            )\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "            self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        def forward(self, x):\n",
        "\n",
        "            out, _ = self.rnn(x)\n",
        "\n",
        "            out = self.dropout(out[:, -1, :])\n",
        "\n",
        "            return self.fc(out)\n",
        "\n",
        "def build_model(hp: Dict, n_vars: int) -> nn.Module:\n",
        "    if not nn: return None\n",
        "    mdl = GRUForecaster(\n",
        "        input_size=n_vars,\n",
        "        hidden_size=int(hp[\"hidden_size\"]),\n",
        "        output_size=1,\n",
        "        dropout=float(hp[\"dropout_rate\"]),\n",
        "        num_layers=int(hp[\"num_layers\"]),\n",
        "    ).to(device)\n",
        "    try:\n",
        "\n",
        "        if hasattr(torch, 'compile'):\n",
        "             return torch.compile(mdl, mode=\"reduce-overhead\")\n",
        "        return mdl\n",
        "    except Exception:\n",
        "        return mdl\n",
        "\n",
        "def smape_loss_np(y_true, y_pred, eps: float = EPSILON) -> float:\n",
        "    y_true = np.asarray(y_true); y_pred = np.asarray(y_pred)\n",
        "    num = np.abs(y_true - y_pred)\n",
        "    den = (np.abs(y_true) + np.abs(y_pred) + eps) / 2.0\n",
        "    return np.mean(num / den)\n",
        "\n",
        "def calculate_metrics(y_true: np.ndarray, y_pred: np.ndarray, insample: Optional[np.ndarray] = None):\n",
        "    if y_true.ndim == 1 or (y_true.ndim > 1 and y_true.shape[1] == 1):\n",
        "        y_true = y_true.ravel(); y_pred = y_pred.ravel()\n",
        "\n",
        "    smape = smape_loss_np(y_true, y_pred)\n",
        "    mase = rmsse = np.nan\n",
        "\n",
        "    if insample is not None and len(insample) > SEASONAL_PERIOD:\n",
        "        d = np.mean(np.abs(np.diff(insample, n=SEASONAL_PERIOD)))\n",
        "        mae = np.mean(np.abs(y_true - y_pred)); mse = np.mean((y_true - y_pred) ** 2)\n",
        "        mase = mae / (d + EPSILON)\n",
        "        denom_rmsse = np.mean((np.diff(insample, n=SEASONAL_PERIOD))**2)\n",
        "        rmsse = np.sqrt(mse / (denom_rmsse + EPSILON))\n",
        "\n",
        "    return {\"SMAPE\": smape, \"MASE\": mase, \"RMSSE\": rmsse}\n",
        "\n",
        "def diebold_mariano_test(actuals: np.ndarray, pred1: np.ndarray, pred2: np.ndarray, horizon: int):\n",
        "\n",
        "    actuals = actuals.ravel(); pred1 = pred1.ravel(); pred2 = pred2.ravel()\n",
        "    if len(actuals) != len(pred1) or len(actuals) != len(pred2): return np.nan, np.nan\n",
        "    N = len(actuals)\n",
        "    if N == 0: return np.nan, np.nan\n",
        "\n",
        "    loss1 = (actuals - pred1)**2; loss2 = (actuals - pred2)**2\n",
        "    d = loss1 - loss2; d_mean = np.mean(d)\n",
        "\n",
        "    q = horizon; gamma = np.zeros(q + 1)\n",
        "    for k in range(q + 1):\n",
        "        if k == 0: gamma[k] = np.var(d, ddof=0)\n",
        "        else:\n",
        "            try:\n",
        "                cov = np.cov(d[k:], d[:N-k], ddof=0)\n",
        "                if cov.ndim == 2: gamma[k] = cov[0, 1]\n",
        "                else: gamma[k] = 0\n",
        "            except Exception: gamma[k] = 0\n",
        "\n",
        "    V = gamma[0] + 2 * np.sum([(1 - k/(q+1)) * gamma[k] for k in range(1, q+1)])\n",
        "\n",
        "    if V <= 1e-9:\n",
        "        if abs(d_mean) < 1e-9: return 0, 1.0\n",
        "        else: return np.inf * np.sign(d_mean), 0.0\n",
        "\n",
        "    DM_stat = d_mean / np.sqrt(V / N)\n",
        "    p_value = 2 * (1 - t.cdf(np.abs(DM_stat), df=N-1))\n",
        "    return DM_stat, p_value\n",
        "\n",
        "\n",
        "def fit_smoother(series, method, alpha=None, beta=None):\n",
        "    if method == \"NS\" or SimpleExpSmoothing is None or Holt is None: return None\n",
        "    try:\n",
        "        if method == \"DES\":\n",
        "            return Holt(series, initialization_method=\"estimated\").fit(smoothing_level=alpha, smoothing_trend=beta, optimized=False)\n",
        "        if method == \"ES\":\n",
        "             return SimpleExpSmoothing(series, initialization_method=\"estimated\").fit(smoothing_level=alpha, optimized=False)\n",
        "    except Exception: return None\n",
        "    return None\n",
        "\n",
        "def apply_fitted_smoother(fitter, df: pd.DataFrame, target: str) -> pd.DataFrame:\n",
        "    if fitter is None: return df\n",
        "    n = len(df); fitted_vals = fitter.fittedvalues\n",
        "    if len(fitted_vals) == 0: return df\n",
        "\n",
        "    if len(fitted_vals) == n: values = fitted_vals\n",
        "    elif len(fitted_vals) < n: values = pd.concat([df[target].iloc[:n-len(fitted_vals)], fitted_vals])\n",
        "    else: values = fitted_vals[:n]\n",
        "\n",
        "    out = df.copy()\n",
        "    if len(values) == n: out[target] = values.values\n",
        "    else: return df\n",
        "\n",
        "    out[target].bfill(inplace=True)\n",
        "    return out\n",
        "\n",
        "\n",
        "def create_sequences_1step(df_in: pd.DataFrame, target: str,\n",
        "                           features: List[str], num_lags: int) -> Tuple[np.ndarray, np.ndarray, List[str]]:\n",
        "    \"\"\"\n",
        "    Creates input sequences (X) and target values (Y) for 1-step ahead forecasting.\n",
        "\n",
        "    X: (N_samples, num_lags, N_features)\n",
        "    Y: (N_samples, 1)\n",
        "    \"\"\"\n",
        "    cols = [target] + features\n",
        "    data = df_in[cols].values\n",
        "    N = data.shape[0]\n",
        "\n",
        "    X, Y = [], []\n",
        "\n",
        "\n",
        "    for i in range(N - num_lags):\n",
        "\n",
        "        x_seq = data[i : i + num_lags]\n",
        "\n",
        "        y_val = data[i + num_lags, 0]\n",
        "\n",
        "        X.append(x_seq)\n",
        "        Y.append(y_val)\n",
        "\n",
        "    return np.array(X), np.array(Y).reshape(-1, 1), cols\n",
        "\n",
        "\n",
        "def select_mrmr_features(df_window: pd.DataFrame, target: str, k: int, num_lags: int) -> List[str]:\n",
        "\n",
        "    if k == 0 or mrmr is None: return []\n",
        "    lagged = []; potential_features = [c for c in df_window.columns if c != target]\n",
        "    if not potential_features: return []\n",
        "\n",
        "    for col in [target] + potential_features:\n",
        "        for l in range(1, num_lags + 1):\n",
        "            lagged.append(df_window[col].shift(l).rename(f\"{col}_lag_{l}\"))\n",
        "\n",
        "    df_lagged = pd.concat([df_window[[target]], *lagged], axis=1).dropna()\n",
        "    if df_lagged.empty: return []\n",
        "\n",
        "    X_cols = [c for c in df_lagged.columns if \"_lag_\" in c and not c.startswith(f\"{target}_lag_\")]\n",
        "    if not X_cols: return []\n",
        "\n",
        "    X = df_lagged[X_cols].ffill().fillna(0); y = df_lagged[target].values.ravel()\n",
        "    K_eff = min(k, len(X_cols))\n",
        "\n",
        "    try:\n",
        "        selected_lagged = mrmr.mrmr_regression(X=X, y=y, K=K_eff)\n",
        "        selected_base = list({s.split(\"_lag_\")[0] for s in selected_lagged})\n",
        "        return selected_base\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "def mc_pred(model: nn.Module, inp: torch.Tensor, mc_samples: int):\n",
        "    \"\"\"Generates Monte Carlo predictions if dropout is active. Ensures output is (mc_samples, 1).\"\"\"\n",
        "    if mc_samples == 1:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            preds = model(inp).cpu().numpy()\n",
        "\n",
        "        preds = np.asarray(preds)\n",
        "        if preds.ndim == 1:\n",
        "            preds = preds.reshape(-1, 1)\n",
        "        elif preds.ndim > 2:\n",
        "            preds = preds.reshape(preds.shape[0], -1)[:, :1]\n",
        "        return preds\n",
        "\n",
        "    model.train()\n",
        "    with torch.no_grad():\n",
        "        inp_mc = inp.repeat(mc_samples, 1, 1)\n",
        "        preds = model(inp_mc).cpu().numpy()\n",
        "    preds = np.asarray(preds)\n",
        "\n",
        "    if preds.ndim == 1:\n",
        "        preds = preds.reshape(-1, 1)\n",
        "    elif preds.ndim > 2:\n",
        "        preds = preds.reshape(preds.shape[0], -1)[:, :1]\n",
        "    elif preds.shape[1] != 1:\n",
        "        preds = preds[:, :1]\n",
        "    return preds\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def make_objective(tgt: str, train_val_df: pd.DataFrame,\n",
        "                   k: int, mode: str):\n",
        "\n",
        "\n",
        "    tscv = TimeSeriesSplit(n_splits=N_CV_SPLITS, test_size=VALIDATION_SIZE_HPO)\n",
        "\n",
        "    def objective(trial: optuna.Trial):\n",
        "\n",
        "        hp = {\n",
        "            \"hidden_size\": trial.suggest_categorical(\"hidden_size\", [64, 128, 256, 512]),\n",
        "            \"num_layers\": trial.suggest_int(\"num_layers\", 1, 3),\n",
        "            \"dropout_rate\": trial.suggest_float(\"dropout_rate\", 0.1, 0.5, step=0.1),\n",
        "            \"lr\": trial.suggest_categorical(\"lr\", [1e-4, 5e-4, 1e-3]),\n",
        "            \"batch_size\": trial.suggest_categorical(\"batch_size\", [32, 64, 128]),\n",
        "            \"n_lags\": trial.suggest_int(\"n_lags\", SEASONAL_PERIOD, SEASONAL_PERIOD * 4, step=SEASONAL_PERIOD),\n",
        "            \"smoothing_method\": trial.suggest_categorical(\"smoothing_method\", [\"NS\", \"ES\", \"DES\"]),\n",
        "            \"smoothing_alpha\": trial.suggest_float(\"smoothing_alpha\", 0.1, 0.6),\n",
        "            \"smoothing_beta\": trial.suggest_float(\"smoothing_beta\", 0.05, 0.6),\n",
        "        }\n",
        "\n",
        "        n_lags = hp[\"n_lags\"]\n",
        "        smapes_cv = []\n",
        "\n",
        "\n",
        "        for tr_idx, val_idx in tscv.split(train_val_df):\n",
        "            if len(tr_idx) < n_lags + 1:\n",
        "                continue\n",
        "\n",
        "            df_tr = train_val_df.iloc[tr_idx]\n",
        "\n",
        "\n",
        "            val_start_idx = val_idx[0] - n_lags\n",
        "            if val_start_idx < 0:\n",
        "                 continue\n",
        "\n",
        "            df_va_context = train_val_df.iloc[val_start_idx : val_idx[-1] + 1]\n",
        "\n",
        "\n",
        "\n",
        "            smoother = fit_smoother(df_tr[tgt], hp[\"smoothing_method\"], hp[\"smoothing_alpha\"], hp[\"smoothing_beta\"])\n",
        "            df_tr_sm = apply_fitted_smoother(smoother, df_tr, tgt)\n",
        "\n",
        "\n",
        "            df_va_context_sm = apply_fitted_smoother(smoother, df_va_context, tgt)\n",
        "\n",
        "\n",
        "\n",
        "            if mode == \"MULTI\":\n",
        "                base_feats = select_mrmr_features(df_tr_sm, tgt, k, num_lags=SEASONAL_PERIOD)\n",
        "            else:\n",
        "                base_feats = []\n",
        "\n",
        "            n_vars = len([tgt, *base_feats])\n",
        "\n",
        "\n",
        "            X_tr, Y_tr, _ = create_sequences_1step(df_tr_sm, tgt, base_feats, n_lags)\n",
        "\n",
        "            X_va, Y_va, _ = create_sequences_1step(df_va_context_sm, tgt, base_feats, n_lags)\n",
        "\n",
        "\n",
        "            if X_tr.shape[0] == 0 or X_va.shape[0] == 0:\n",
        "                continue\n",
        "\n",
        "\n",
        "            X_tr_reshaped = X_tr.reshape(-1, n_vars)\n",
        "            sc_X = MinMaxScaler().fit(X_tr_reshaped); sc_y = MinMaxScaler().fit(Y_tr)\n",
        "\n",
        "            X_tr_scaled = sc_X.transform(X_tr.reshape(-1, n_vars)).reshape(X_tr.shape)\n",
        "            Y_tr_scaled = sc_y.transform(Y_tr)\n",
        "            X_va_scaled = sc_X.transform(X_va.reshape(-1, n_vars)).reshape(X_va.shape)\n",
        "\n",
        "\n",
        "\n",
        "            Xtr_t = torch.tensor(X_tr_scaled, dtype=torch.float32)\n",
        "            Ytr_t = torch.tensor(Y_tr_scaled, dtype=torch.float32)\n",
        "            Xva_t = torch.tensor(X_va_scaled, dtype=torch.float32).to(device)\n",
        "\n",
        "            dl = torch.utils.data.DataLoader(\n",
        "                torch.utils.data.TensorDataset(Xtr_t, Ytr_t),\n",
        "                batch_size=hp[\"batch_size\"], shuffle=True)\n",
        "\n",
        "\n",
        "            mdl = build_model(hp, n_vars)\n",
        "            crit = nn.L1Loss()\n",
        "            opt = optim.AdamW(mdl.parameters(), lr=hp[\"lr\"], weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "            best_fold_smape = float(\"inf\")\n",
        "            no_imp = 0\n",
        "\n",
        "            for ep in range(MAX_EPOCHS_OPTUNA):\n",
        "                mdl.train()\n",
        "                for xb, yb in dl:\n",
        "                    xb, yb = xb.to(device), yb.to(device)\n",
        "                    opt.zero_grad(set_to_none=True)\n",
        "                    with torch.autocast(device.type, enabled=(device.type == \"cuda\")):\n",
        "                        preds = mdl(xb)\n",
        "\n",
        "                        loss = crit(preds, yb)\n",
        "\n",
        "                    if torch.isnan(loss): return float(\"inf\")\n",
        "\n",
        "                    loss.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(mdl.parameters(), 1.0)\n",
        "                    opt.step()\n",
        "\n",
        "\n",
        "                mdl.eval()\n",
        "                with torch.no_grad():\n",
        "                    preds_scaled = mdl(Xva_t).cpu().numpy()\n",
        "\n",
        "                    preds_inv = sc_y.inverse_transform(preds_scaled)\n",
        "\n",
        "\n",
        "                    s = smape_loss_np(Y_va, preds_inv)\n",
        "\n",
        "\n",
        "                trial.report(s, ep)\n",
        "                if trial.should_prune():\n",
        "                    raise optuna.TrialPruned()\n",
        "\n",
        "                if s < best_fold_smape - 1e-4:\n",
        "                    best_fold_smape = s; no_imp = 0\n",
        "                else:\n",
        "                    no_imp += 1\n",
        "\n",
        "                if no_imp >= PATIENCE_ES:\n",
        "                    break\n",
        "\n",
        "            smapes_cv.append(best_fold_smape)\n",
        "\n",
        "        if not smapes_cv:\n",
        "            return float(\"inf\")\n",
        "\n",
        "\n",
        "        return float(np.mean(smapes_cv))\n",
        "\n",
        "    return objective\n",
        "\n",
        "\n",
        "\n",
        "def train_final_model(hp: Dict, df_train: pd.DataFrame, tgt: str, base_feats: List[str], use_early_stopping: bool = True):\n",
        "    \"\"\"\n",
        "    Trains the 1-step LSTM model with optimized hyperparameters.\n",
        "    \"\"\"\n",
        "    n_lags = hp[\"n_lags\"]\n",
        "    n_vars = len([tgt, *base_feats])\n",
        "\n",
        "\n",
        "    smoother = fit_smoother(df_train[tgt], hp.get(\"smoothing_method\", \"NS\"), hp.get(\"smoothing_alpha\"), hp.get(\"smoothing_beta\"))\n",
        "    df_train_sm = apply_fitted_smoother(smoother, df_train, tgt)\n",
        "\n",
        "\n",
        "    X, Y, _ = create_sequences_1step(df_train_sm, tgt, base_feats, n_lags)\n",
        "    if X.shape[0] == 0: return None, None, None\n",
        "\n",
        "\n",
        "    if use_early_stopping and X.shape[0] > 20:\n",
        "         split_idx = int(X.shape[0] * 0.8)\n",
        "         X_tr, Y_tr = X[:split_idx], Y[:split_idx]\n",
        "         X_va, Y_va = X[split_idx:], Y[split_idx:]\n",
        "    else:\n",
        "        X_tr, Y_tr = X, Y\n",
        "        X_va, Y_va = None, None\n",
        "\n",
        "\n",
        "    X_tr_reshaped = X_tr.reshape(-1, n_vars)\n",
        "    sc_X = MinMaxScaler().fit(X_tr_reshaped); sc_y = MinMaxScaler().fit(Y_tr)\n",
        "\n",
        "    X_tr_scaled = sc_X.transform(X_tr.reshape(-1, n_vars)).reshape(X_tr.shape)\n",
        "    Y_tr_scaled = sc_y.transform(Y_tr)\n",
        "\n",
        "    Xtr_t = torch.tensor(X_tr_scaled, dtype=torch.float32)\n",
        "    Ytr_t = torch.tensor(Y_tr_scaled, dtype=torch.float32)\n",
        "\n",
        "    if X_va is not None:\n",
        "        X_va_scaled = sc_X.transform(X_va.reshape(-1, n_vars)).reshape(X_va.shape)\n",
        "        Y_va_scaled = sc_y.transform(Y_va)\n",
        "        Xva_t = torch.tensor(X_va_scaled, dtype=torch.float32).to(device)\n",
        "        Yva_t = torch.tensor(Y_va_scaled, dtype=torch.float32).to(device)\n",
        "    else:\n",
        "        Xva_t, Yva_t = None, None\n",
        "\n",
        "    dl = torch.utils.data.DataLoader(\n",
        "        torch.utils.data.TensorDataset(Xtr_t, Ytr_t),\n",
        "        batch_size=hp[\"batch_size\"], shuffle=True)\n",
        "\n",
        "\n",
        "    mdl = build_model(hp, n_vars)\n",
        "    if mdl is None: return None, None, None\n",
        "\n",
        "    crit = nn.L1Loss()\n",
        "    opt = optim.AdamW(mdl.parameters(), lr=hp[\"lr\"], weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "\n",
        "    best_val_loss = float(\"inf\"); no_imp = 0; best_model_state = None\n",
        "\n",
        "    if Xva_t is None:\n",
        "         best_model_state = copy.deepcopy(mdl.state_dict())\n",
        "\n",
        "    for ep in range(MAX_EPOCHS_FINAL):\n",
        "        mdl.train()\n",
        "        for xb, yb in dl:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with torch.autocast(device.type, enabled=(device.type == \"cuda\")):\n",
        "                preds = mdl(xb)\n",
        "                loss = crit(preds, yb)\n",
        "\n",
        "            if torch.isnan(loss): return None, None, None\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(mdl.parameters(), 1.0)\n",
        "            opt.step()\n",
        "\n",
        "\n",
        "        if Xva_t is not None:\n",
        "            mdl.eval()\n",
        "            with torch.no_grad():\n",
        "                 with torch.autocast(device.type, enabled=(device.type == \"cuda\")):\n",
        "                    val_preds = mdl(Xva_t)\n",
        "                    val_loss = crit(val_preds, Yva_t).item()\n",
        "\n",
        "            if val_loss < best_val_loss - 1e-4:\n",
        "                best_val_loss = val_loss; no_imp = 0\n",
        "                best_model_state = copy.deepcopy(mdl.state_dict())\n",
        "            else:\n",
        "                no_imp += 1\n",
        "\n",
        "            if no_imp >= PATIENCE_ES: break\n",
        "\n",
        "    if best_model_state:\n",
        "         mdl.load_state_dict(best_model_state)\n",
        "\n",
        "    return mdl, sc_X, sc_y\n",
        "\n",
        "\n",
        "def run_hpo_pipeline(df_dev: pd.DataFrame, targets: List[str], k: int, mode: str) -> Dict:\n",
        "    best_cfgs = {}\n",
        "    print(f\"\\n--- Starting HPO Pipeline (Model: {MODEL_TYPE}, Mode: {mode}) ---\")\n",
        "\n",
        "    for tgt in targets:\n",
        "        print(f\"\\nOptimizing Target: {tgt}\")\n",
        "\n",
        "        study_name = f\"{MODEL_TYPE}_{mode}_{tgt}_HPO\"\n",
        "        try:\n",
        "            study = optuna.create_study(\n",
        "                study_name=study_name,\n",
        "                direction=\"minimize\",\n",
        "                sampler=TPESampler(seed=RANDOM_STATE),\n",
        "                pruner=SuccessiveHalvingPruner(min_resource=4, reduction_factor=3),\n",
        "                storage=STORAGE,\n",
        "                load_if_exists=True\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Could not create/load study: {e}\")\n",
        "            continue\n",
        "\n",
        "        if len(study.trials) < OPTUNA_N_TRIALS:\n",
        "            objective_fn = make_objective(tgt, df_dev, k, mode)\n",
        "            try:\n",
        "                study.optimize(\n",
        "                    objective_fn,\n",
        "                    n_trials=OPTUNA_N_TRIALS - len(study.trials),\n",
        "                    n_jobs=1,\n",
        "                    show_progress_bar=True\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"Optimization failed: {e}\")\n",
        "                continue\n",
        "\n",
        "        if study.best_trial is None:\n",
        "            print(f\"HPO failed for {tgt}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Best CV 1-Step SMAPE for {tgt}: {study.best_value:.4f}\")\n",
        "        best_hp = study.best_params\n",
        "\n",
        "\n",
        "        if mode == \"MULTI\":\n",
        "            smoother = fit_smoother(\n",
        "                df_dev[tgt],\n",
        "                best_hp.get(\"smoothing_method\", \"NS\"),\n",
        "                best_hp.get(\"smoothing_alpha\", 0.5),\n",
        "                best_hp.get(\"smoothing_beta\", 0.1)\n",
        "            )\n",
        "            df_dev_sm = apply_fitted_smoother(smoother, df_dev, tgt)\n",
        "            final_feats = select_mrmr_features(df_dev_sm, tgt, k, num_lags=SEASONAL_PERIOD)\n",
        "        else:\n",
        "            final_feats = []\n",
        "\n",
        "        best_cfgs[tgt] = {\n",
        "            \"smape_dev_1step\": study.best_value,\n",
        "            \"hyperparams\": best_hp,\n",
        "            \"base_features\": final_feats,\n",
        "            \"n_vars_per_step\": len([tgt, *final_feats]),\n",
        "            \"n_lags\": best_hp[\"n_lags\"]\n",
        "        }\n",
        "\n",
        "    return best_cfgs\n",
        "\n",
        "\n",
        "\n",
        "def forecast_arima(series: pd.Series, horizon: int):\n",
        "    if pm is None: return np.full(horizon, np.nan)\n",
        "    try:\n",
        "        model = pm.auto_arima(series, start_p=1, start_q=1, max_p=5, max_q=5, m=SEASONAL_PERIOD,\n",
        "                              start_P=0, seasonal=True, d=None, D=1, trace=False,\n",
        "                              error_action='ignore', suppress_warnings=True, stepwise=True)\n",
        "        forecast = model.predict(n_periods=horizon)\n",
        "        return np.asarray(forecast)\n",
        "    except Exception: return np.full(horizon, np.nan)\n",
        "\n",
        "def forecast_ets(series: pd.Series, horizon: int):\n",
        "    if ETSModel is None: return np.full(horizon, np.nan)\n",
        "    try:\n",
        "        model = ETSModel(series, error=\"add\", trend=\"add\", seasonal=\"add\",\n",
        "                         damped_trend=True, seasonal_periods=SEASONAL_PERIOD)\n",
        "        fit = model.fit(disp=False, optimized=True)\n",
        "        forecast = fit.forecast(horizon)\n",
        "        return np.asarray(forecast)\n",
        "    except Exception:\n",
        "\n",
        "        return np.repeat(series.iloc[-1], horizon)\n",
        "\n",
        "def forecast_exogenous_variables(df_hist: pd.DataFrame, features: List[str], horizon: int) -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Rigorously forecasts future values of exogenous variables using ETS.\n",
        "    \"\"\"\n",
        "    forecasts = {}\n",
        "    for feat in features:\n",
        "\n",
        "        forecasts[feat] = forecast_ets(df_hist[feat], horizon)\n",
        "    return forecasts\n",
        "\n",
        "def _append_next_point(current_input_t: torch.Tensor,\n",
        "                       pred_next_scaled: torch.Tensor,\n",
        "                       n_vars: int,\n",
        "                       policy: str = FUTURE_EXOG_POLICY) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Build the (Batch, 1, n_vars) point to append after predicting the next step,\n",
        "    WITHOUT using any *future* exogenous values.\n",
        "\n",
        "    Inputs are in *scaled* space:\n",
        "      - current_input_t: (B, L, n_vars)\n",
        "      - pred_next_scaled: (B, 1) predicted target (scaled)\n",
        "    \"\"\"\n",
        "    B = current_input_t.size(0)\n",
        "    new_point = pred_next_scaled.unsqueeze(1)\n",
        "\n",
        "    if n_vars == 1:\n",
        "        return new_point\n",
        "\n",
        "    if policy == \"persist\":\n",
        "\n",
        "        exog_last = current_input_t[:, -1:, 1:]\n",
        "        return torch.cat([new_point, exog_last], dim=2)\n",
        "\n",
        "    if policy == \"none\":\n",
        "        pad = torch.zeros((B, 1, n_vars - 1),\n",
        "                          device=current_input_t.device,\n",
        "                          dtype=current_input_t.dtype)\n",
        "        return torch.cat([new_point, pad], dim=2)\n",
        "\n",
        "    if policy == \"forecast\":\n",
        "        raise RuntimeError(\"FUTURE_EXOG_POLICY='forecast' not allowed in Track A. Use 'persist' or 'none'.\")\n",
        "\n",
        "\n",
        "def generate_forecast_recursive(\n",
        "    df_hist: pd.DataFrame, target: str, cfg: Dict, horizon: int,\n",
        "    mc_samples: int):\n",
        "    \"\"\"\n",
        "    Trains the 1-step model and generates a H-step ahead forecast recursively.\n",
        "    \"\"\"\n",
        "    if not cfg or not cfg.get(\"hyperparams\"): return None\n",
        "\n",
        "    hp = cfg[\"hyperparams\"]; feats = cfg[\"base_features\"]\n",
        "    n_lags = cfg[\"n_lags\"]; n_vars = cfg[\"n_vars_per_step\"]\n",
        "\n",
        "\n",
        "    mdl, sc_X, sc_y = train_final_model(hp, df_hist, target, feats, use_early_stopping=True)\n",
        "\n",
        "    if mdl is None: return None\n",
        "\n",
        "\n",
        "    if FUTURE_EXOG_POLICY == \"forecast\" and feats:\n",
        "        exog_forecasts = forecast_exogenous_variables(df_hist, feats, horizon)\n",
        "    else:\n",
        "        exog_forecasts = None\n",
        "\n",
        "\n",
        "\n",
        "    smoother = fit_smoother(df_hist[target], hp.get(\"smoothing_method\", \"NS\"), hp.get(\"smoothing_alpha\"), hp.get(\"smoothing_beta\"))\n",
        "    df_hist_sm = apply_fitted_smoother(smoother, df_hist, target)\n",
        "\n",
        "\n",
        "    input_data = df_hist_sm[[target] + feats].iloc[-n_lags:].values\n",
        "\n",
        "\n",
        "    input_scaled = sc_X.transform(input_data.reshape(-1, n_vars)).reshape(1, n_lags, n_vars)\n",
        "    current_input_t = torch.tensor(input_scaled, dtype=torch.float32, device=device)\n",
        "\n",
        "\n",
        "    use_mc = mc_samples > 1 and hp.get(\"dropout_rate\", 0) > 0\n",
        "    samples = mc_samples if use_mc else 1\n",
        "\n",
        "\n",
        "    all_forecasts = np.zeros((samples, horizon))\n",
        "\n",
        "    for h in range(horizon):\n",
        "\n",
        "        preds_mc_scaled = mc_pred(mdl, current_input_t, samples)\n",
        "\n",
        "        preds_mc_scaled = np.asarray(preds_mc_scaled)\n",
        "        if preds_mc_scaled.ndim == 2:\n",
        "\n",
        "            preds_mc_scaled = preds_mc_scaled.reshape(preds_mc_scaled.shape[0], -1)[:, 0]\n",
        "        else:\n",
        "            preds_mc_scaled = preds_mc_scaled.reshape(-1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        all_forecasts[:, h] = preds_mc_scaled.ravel()\n",
        "\n",
        "\n",
        "        if h < horizon - 1:\n",
        "\n",
        "          if current_input_t.shape[0] == 1 and samples > 1:\n",
        "              current_input_t = current_input_t.repeat(samples, 1, 1)\n",
        "\n",
        "          if exog_forecasts is None:\n",
        "\n",
        "              pred_scaled_t = torch.tensor(preds_mc_scaled, dtype=torch.float32, device=device).unsqueeze(1)\n",
        "              new_point_t = _append_next_point(current_input_t, pred_scaled_t, n_vars, FUTURE_EXOG_POLICY)\n",
        "          else:\n",
        "\n",
        "              new_point = preds_mc_scaled.reshape(samples, 1)\n",
        "\n",
        "\n",
        "              exog_vals_h = np.array([exog_forecasts[f][h] for f in feats]).reshape(1, -1)\n",
        "              dummy_for_scaling = np.hstack([np.zeros((1, 1)), exog_vals_h])\n",
        "              scaled_dummy = sc_X.transform(dummy_for_scaling)\n",
        "              scaled_exog_h = scaled_dummy[:, 1:].reshape(1, len(feats))\n",
        "              scaled_exog_repeated = np.repeat(scaled_exog_h, samples, axis=0)\n",
        "\n",
        "              new_point_np = np.hstack([new_point, scaled_exog_repeated])\n",
        "              new_point_t = torch.tensor(new_point_np.reshape(samples, 1, n_vars),\n",
        "                                        dtype=torch.float32, device=device)\n",
        "\n",
        "\n",
        "          current_input_t = torch.cat([current_input_t[:, 1:, :], new_point_t], dim=1)\n",
        "\n",
        "\n",
        "\n",
        "    preds_inv = sc_y.inverse_transform(all_forecasts.reshape(-1, 1)).reshape(samples, horizon)\n",
        "\n",
        "\n",
        "    if use_mc:\n",
        "        forecasts = np.median(preds_inv, axis=0)\n",
        "        lowers = np.percentile(preds_inv, 2.5, axis=0)\n",
        "        uppers = np.percentile(preds_inv, 97.5, axis=0)\n",
        "    else:\n",
        "        forecasts = preds_inv[0]\n",
        "        lowers = np.full(horizon, np.nan); uppers = np.full(horizon, np.nan)\n",
        "\n",
        "\n",
        "    forecasts[forecasts < 0] = 0\n",
        "    if use_mc:\n",
        "        lowers[lowers < 0] = 0; uppers[uppers < 0] = 0\n",
        "\n",
        "    return pd.DataFrame({\"Forecast\": forecasts, \"Lower_CI\": lowers, \"Upper_CI\": uppers})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def plot_error_horizon_analysis(results_df: pd.DataFrame, model_type: str, output_dir: Path):\n",
        "    \"\"\"Analyzes and plots the SMAPE error across the 36-month horizon.\"\"\"\n",
        "\n",
        "\n",
        "    model_results = results_df[results_df['Model'].str.startswith(model_type)]\n",
        "\n",
        "    if model_results.empty:\n",
        "        print(\"No data for error horizon analysis.\")\n",
        "        return\n",
        "\n",
        "\n",
        "    ape_cols = [f'APE_H{h+1}' for h in range(FORECAST_HORIZON)]\n",
        "\n",
        "    if ape_cols[0] not in model_results.columns:\n",
        "        print(\"APE columns not found in results dataframe. Cannot perform analysis.\")\n",
        "        return\n",
        "\n",
        "\n",
        "    melted_df = pd.melt(model_results, id_vars=['Target', 'Model'], value_vars=ape_cols, var_name='Horizon', value_name='APE')\n",
        "\n",
        "\n",
        "    melted_df['Horizon_Step'] = melted_df['Horizon'].str.replace('APE_H', '').astype(int)\n",
        "\n",
        "\n",
        "    melted_df['SMAPE'] = melted_df['APE'] * 100\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.lineplot(data=melted_df, x='Horizon_Step', y='SMAPE', hue='Model', errorbar='sd')\n",
        "    plt.title(f'Error Horizon Analysis ({model_type} Strategies)')\n",
        "    plt.xlabel('Forecast Horizon (Months)')\n",
        "    plt.ylabel('Average SMAPE (%)')\n",
        "    plt.xticks(range(1, FORECAST_HORIZON + 1, 2))\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.legend(title='Strategy')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plot_path = output_dir / \"error_horizon_analysis.png\"\n",
        "    plt.savefig(plot_path)\n",
        "    print(f\"\\nError Horizon Analysis plot saved to: {plot_path}\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if not torch or not optuna:\n",
        "        print(\"\\nExecution aborted due to missing critical dependencies (PyTorch or Optuna).\")\n",
        "\n",
        "\n",
        "\n",
        "    if ConvergenceWarning: warnings.simplefilter(\"ignore\", ConvergenceWarning)\n",
        "    if ValueWarning: warnings.simplefilter(\"ignore\", ValueWarning)\n",
        "    warnings.simplefilter(\"ignore\", UserWarning)\n",
        "\n",
        "    if optuna:\n",
        "        try: optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "        except Exception: pass\n",
        "\n",
        "    --\n",
        "    DF_PATH = ROOT / \"yourdatahere\"\n",
        "\n",
        "\n",
        "    if DF_PATH.exists():\n",
        "        df_raw = pd.read_excel(DF_PATH)\n",
        "    else:\n",
        "         print(\"Data file not found. Proceeding with MOCK DATA.\")\n",
        "\n",
        "\n",
        "    date_col = next((c for c in df_raw.columns\n",
        "                     if c.lower().strip() == \"date\"), None)\n",
        "    if date_col:\n",
        "        df_raw[date_col] = pd.to_datetime(df_raw[date_col])\n",
        "        df_raw.sort_values(date_col, inplace=True)\n",
        "        df_raw.set_index(date_col, inplace=True)\n",
        "\n",
        "    df_raw.columns = df_raw.columns.str.lower().str.strip()\n",
        "\n",
        "\n",
        "    for col in df_raw.columns:\n",
        "        if df_raw[col].dtype == \"object\":\n",
        "             try:\n",
        "                df_raw[col] = pd.to_numeric(\n",
        "                    df_raw[col].str.replace(r\"[^\\d.\\-]\", \"\", regex=True),\n",
        "                    errors=\"coerce\")\n",
        "             except AttributeError:\n",
        "                 df_raw[col] = pd.to_numeric(df_raw[col], errors=\"coerce\")\n",
        "    df_raw = df_raw.ffill().fillna(0)\n",
        "\n",
        "\n",
        "    df_dev = df_raw.iloc[:-FORECAST_HORIZON].copy()\n",
        "    df_test = df_raw.iloc[-FORECAST_HORIZON:].copy()\n",
        "\n",
        "\n",
        "    targets = [col for col in df_raw.columns if 'journal article' in col]\n",
        "    if not targets:\n",
        "            raise RuntimeError(\"No valid targets found in data.\")\n",
        "\n",
        "\n",
        "    K_FEATURES = 10\n",
        "    best_cfgs_modes = {}\n",
        "\n",
        "\n",
        "    for MODE in [\"UNI\", \"MULTI\"]:\n",
        "        if MODE == \"MULTI\" and (mrmr is None or len(df_raw.columns) < 2):\n",
        "            print(f\"Skipping {MODE} mode.\"); continue\n",
        "\n",
        "\n",
        "        best_cfgs = run_hpo_pipeline(df_dev, targets, k=K_FEATURES, mode=MODE)\n",
        "        best_cfgs_modes[MODE] = best_cfgs\n",
        "\n",
        "\n",
        "        best_path = BASE_OUT / f\"best_hyperparameters_{MODE}.json\"\n",
        "        try:\n",
        "\n",
        "            tmp = best_path.with_suffix(\".tmp\")\n",
        "            with open(tmp, 'w') as f:\n",
        "                json.dump(best_cfgs, f, indent=2)\n",
        "            tmp.replace(best_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Could not save HPO results for {MODE}: {e}\")\n",
        "\n",
        "    print(\"\\n--- Starting Test Set (Hold-out) Evaluation ---\")\n",
        "    test_results = []; baseline_cache = {}\n",
        "\n",
        "\n",
        "    evaluation_records = []\n",
        "\n",
        "    for MODE in best_cfgs_modes.keys():\n",
        "        best_cfgs = best_cfgs_modes[MODE]\n",
        "        MODEL_MODE_NAME = f\"{MODEL_TYPE}_{MODE}\"\n",
        "\n",
        "        for tgt in targets:\n",
        "            cfg = best_cfgs.get(tgt)\n",
        "            if not cfg: continue\n",
        "\n",
        "\n",
        "\n",
        "            fc_df = generate_forecast_recursive(df_dev, tgt, cfg, FORECAST_HORIZON, mc_samples=1)\n",
        "\n",
        "            if fc_df is None:\n",
        "                print(f\"Skipping {tgt} ({MODE}) due to model training failure.\"); continue\n",
        "\n",
        "            y_true = df_test[tgt].values.ravel()\n",
        "            y_pred_model = fc_df[\"Forecast\"].values.ravel()\n",
        "            insample_data = df_dev[tgt].values\n",
        "\n",
        "            model_m = calculate_metrics(y_true, y_pred_model, insample_data)\n",
        "\n",
        "\n",
        "            ape = np.abs(y_true - y_pred_model) / ((np.abs(y_true) + np.abs(y_pred_model) + EPSILON) / 2.0)\n",
        "            ape_dict = {f'APE_H{h+1}': ape[h] for h in range(FORECAST_HORIZON)}\n",
        "\n",
        "\n",
        "            record = {\n",
        "                \"Target\": tgt, \"Model\": MODEL_MODE_NAME,\n",
        "                **{f\"{k}\": v for k, v in model_m.items()},\n",
        "                **ape_dict\n",
        "            }\n",
        "\n",
        "\n",
        "            if tgt not in baseline_cache:\n",
        "                print(f\"Calculating Baselines for {tgt}...\")\n",
        "\n",
        "                arima_pred = forecast_arima(df_dev[tgt], FORECAST_HORIZON)\n",
        "                ets_pred = forecast_ets(df_dev[tgt], FORECAST_HORIZON)\n",
        "                baseline_cache[tgt] = {\"ARIMA\": arima_pred, \"ETS\": ets_pred}\n",
        "\n",
        "\n",
        "            for name, pred in baseline_cache[tgt].items():\n",
        "                 if np.isnan(pred).all():\n",
        "                     metrics = {\"SMAPE\": np.nan, \"MASE\": np.nan, \"RMSSE\": np.nan}\n",
        "                     ape_dict_base = {f'APE_H{h+1}': np.nan for h in range(FORECAST_HORIZON)}\n",
        "                 else:\n",
        "                    metrics = calculate_metrics(y_true, pred, insample_data)\n",
        "                    ape_base = np.abs(y_true - pred) / ((np.abs(y_true) + np.abs(pred) + EPSILON) / 2.0)\n",
        "                    ape_dict_base = {f'APE_H{h+1}': ape_base[h] for h in range(FORECAST_HORIZON)}\n",
        "\n",
        "\n",
        "                 baseline_record = {\n",
        "                     \"Target\": tgt, \"Model\": name,\n",
        "                     **metrics,\n",
        "                     **ape_dict_base\n",
        "                 }\n",
        "                 evaluation_records.append(baseline_record)\n",
        "\n",
        "\n",
        "            arima_smape = evaluation_records[-2]['SMAPE'] if len(evaluation_records) >= 2 else np.inf\n",
        "            ets_smape = evaluation_records[-1]['SMAPE'] if len(evaluation_records) >= 1 else np.inf\n",
        "\n",
        "            if np.isnan(arima_smape): arima_smape = np.inf\n",
        "            if np.isnan(ets_smape): ets_smape = np.inf\n",
        "\n",
        "            best_stat_name = None\n",
        "            if arima_smape != np.inf or ets_smape != np.inf:\n",
        "                if arima_smape <= ets_smape:\n",
        "                    best_stat_name = \"ARIMA\"; best_stat_pred = baseline_cache[tgt][\"ARIMA\"]\n",
        "                else:\n",
        "                    best_stat_name = \"ETS\"; best_stat_pred = baseline_cache[tgt][\"ETS\"]\n",
        "\n",
        "            if best_stat_name:\n",
        "                if np.isnan(y_pred_model).any() or np.isnan(best_stat_pred).any():\n",
        "                    dm_stat, dm_p = np.nan, np.nan\n",
        "                else:\n",
        "                    dm_stat, dm_p = diebold_mariano_test(y_true, y_pred_model, best_stat_pred, FORECAST_HORIZON)\n",
        "\n",
        "                record[f\"DM_vs_{best_stat_name}_Stat\"] = dm_stat\n",
        "                record[f\"DM_vs_{best_stat_name}_Pval\"] = dm_p\n",
        "\n",
        "            evaluation_records.append(record)\n",
        "\n",
        "\n",
        "    df_results = pd.DataFrame(evaluation_records)\n",
        "\n",
        "    if not df_results.empty:\n",
        "\n",
        "        df_results.to_csv(BASE_OUT / \"test_evaluation_detailed.csv\", index=False)\n",
        "\n",
        "\n",
        "        try:\n",
        "            summary_metrics = ['SMAPE', 'MASE', 'RMSSE']\n",
        "\n",
        "            dm_cols = [col for col in df_results.columns if col.startswith('DM_')]\n",
        "\n",
        "            pivot_df = df_results.pivot_table(index='Target', columns='Model', values=summary_metrics + dm_cols)\n",
        "\n",
        "\n",
        "            pivot_df.columns = ['_'.join(col).strip() for col in pivot_df.columns.values]\n",
        "            pivot_df = pivot_df.reset_index()\n",
        "            pivot_df.to_csv(BASE_OUT / \"test_evaluation_summary.csv\", index=False)\n",
        "            print(\"\\nTest Evaluation Summary saved.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating summary pivot table: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "        print(\"\\n--- Performing Error Horizon Analysis ---\")\n",
        "        plot_error_horizon_analysis(df_results, MODEL_TYPE, BASE_OUT)\n",
        "\n",
        "\n",
        "\n",
        "    print(\"\\n--- Generating Final 36-Month Future Forecasts (using full data) ---\")\n",
        "    df_full = pd.concat([df_dev, df_test])\n",
        "\n",
        "    for MODE, best_cfgs in best_cfgs_modes.items():\n",
        "        for tgt in targets:\n",
        "            cfg = best_cfgs.get(tgt)\n",
        "            if not cfg: continue\n",
        "\n",
        "\n",
        "            fc_df = generate_forecast_recursive(df_full, tgt, cfg, FORECAST_HORIZON, MC_SAMPLES_FOR_CI)\n",
        "\n",
        "            if fc_df is None:\n",
        "                print(f\"Skipping future projection for {tgt} ({MODE}).\"); continue\n",
        "\n",
        "\n",
        "            safe_name = \"\".join(c if c.isalnum() else \"_\" for c in tgt)\n",
        "            fc_df.to_csv(BASE_OUT / f\"forecast_future_36m_{MODE.lower()}_{safe_name}.csv\", index=False)\n",
        "\n",
        "    print(f\"\\nAll tasks finished successfully. Outputs are located at: {BASE_OUT}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DSIiy-_mkpJ0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}